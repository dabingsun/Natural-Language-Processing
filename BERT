woer2vec、bert、介绍对比：https://www.cnblogs.com/rucwxb/p/10277217.html     http://www.sohu.com/a/322817228_294584
快速认识bert: https://www.jianshu.com/p/d110d0c13063
全称：Bidirectional Encoder Representation from Transformers
bert源码解读：https://www.jianshu.com/p/22e462f01d8c

自然语言处理：
第一步，需要将语言（英语、汉语等）转化为计算机可以识别的机器语言（数字或向量表示）。

1：one-hot编码（比如：词袋模型bag of word）
两句话：1：我和小明去游泳  2：小明和小刚去游泳---> 建立一个包含主要关键词的词典，有6个词，{我：1，和：2，小明：3，去：4，游泳：5，小刚：6}，
则使用one-hot编码后向量表示为：我和小明去游泳 [111110]，小明和小刚去游泳[011111]，每个句子的表示都是词典的一个映射，大小为词典的长度，
当前句子中是否包含词典的词，包含则表示为1，否则就为0。
优点：1 将词向量映射为了机器语言
2 每句的词向量大小一致
缺点：
1 语料多的时候，会导致词典过大，特定单词只出现一次，导致表示向量高维稀疏
2 由于是按照词典方式，缺乏语境，无法表示词之间的上下文关系
